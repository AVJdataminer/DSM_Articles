<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>2019-04-14-DSM_PREP</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="the-data-science-method-dsm--pre-processing-and-training-data-development">The Data Science Method (DSM) -Pre-processing and Training Data Development</h1>
<p><a href="https://medium.com/@aiden.dataminer?source=post_page-----fd2d75182967----------------------"></a></p>
<p><img src="https://miro.medium.com/fit/c/96/96/1*WDVszdcJs5A1fXhnjHZPig.jpeg" alt="Aiden V Johnson"></p>
<p><a href="https://medium.com/@aiden.dataminer?source=post_page-----fd2d75182967----------------------">Aiden V Johnson</a></p>
<p><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-pre-processing-and-training-data-development-fd2d75182967?source=post_page-----fd2d75182967----------------------">Apr 14, 2019</a>  ·  5  min read</p>
<p><img src="https://miro.medium.com/max/60/0*asoa4AWtZhShY7nd?q=20" alt=""></p>
<p><img src="https://miro.medium.com/max/6000/0*asoa4AWtZhShY7nd" alt=""></p>
<p>Photo by  <a href="https://unsplash.com/@kjarrett?utm_source=medium&amp;utm_medium=referral">Kevin Jarrett</a>  on  <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></p>
<p>This is the fourth article in a series about how to take your data science projects to the next level by using a methodological approach similar to the scientific method coined the Data Science Method. This article is focused on the pre-processing of model development dataset and training data development. If you missed the previous article(s) in this series, you can go to the beginning  <a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-a-framework-on-how-to-take-your-data-science-projects-to-the-next-91f9fd81e5d1">here</a>, or click on each step title below to read a specific step in the process.</p>
<p><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-a-framework-on-how-to-take-your-data-science-projects-to-the-next-91f9fd81e5d1"><strong>The Data Science Method</strong></a></p>
<ol>
<li><a href="https://medium.com/@aiden.dataminer/the-data-science-method-problem-identification-6ffcda1e5152?source=friends_link&amp;sk=a56dc93c270bfdd21a585b0618984be0">Problem Identification</a></li>
<li><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-data-collection-organization-and-definitions-d19b6ff141c4">Data Collection, Organization, and Definitions</a></li>
<li><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-exploratory-data-analysis-bc84d4d8d3f9">Exploratory Data Analysis</a></li>
<li>Pre-processing and Training Data Development</li>
<li><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-modeling-56b4233cad1b">Modeling</a></li>
<li><a href="https://medium.com/@aiden.dataminer/the-data-science-method-dsm-documentation-c92c28bd45e6">Documentation</a></li>
</ol>
<p><img src="https://miro.medium.com/max/60/1*MFB23_y8rXJek6p7vav1tg.png?q=20" alt=""></p>
<p><img src="https://miro.medium.com/max/5000/1*MFB23_y8rXJek6p7vav1tg.png" alt=""></p>
<p>Pre-processing is the concept of standardizing your model development dataset. This is applied in situations where you have differences in the magnitude of numeric features and situations where you have categorical and continuous variables. This would also be the juncture where other numeric translation would be applied to meet some scientific assumptions about the feature, such as accounting for atmospheric attenuation in satellite imagery data.</p>
<p>Here are the general steps in pre-processing and training data development:</p>
<ol>
<li><strong>Create dummy or indicator features for categorical variables</strong></li>
<li><strong>Standardize the magnitude of numeric features</strong></li>
<li><strong>Split into testing and training datasets</strong></li>
</ol>
<hr>
<ol>
<li><strong>Create dummy or indicator features for categorical variables</strong></li>
</ol>
<p>Although some machine learning algorithms can interpret multi-level categorical variables, many machine learning models cannot handle categorical variables unless they are converted to dummy variables. I hate that term, ‘dummy variables’. Specifically, the variable is converted into a series of boolean variables for each level of a categorical feature. I first learned this concept as an indicator variable, as it indicates the presence or absence of something. For example, below we have the vehicle data set with three categorical columns; specifically, Manufacturer, Model, and vehicle type. We need to create an indicator column of each level of the manufacturer.</p>
<p><img src="https://miro.medium.com/max/60/1*y2lFEEFFyp6cyTQaCyC8Bw.png?q=20" alt=""></p>
<p><img src="https://miro.medium.com/max/390/1*y2lFEEFFyp6cyTQaCyC8Bw.png" alt=""></p>
<p>An original data frame with categorical features</p>
<p>First, we select all the columns that are categorical which are those with the data type = ‘object’, creating a data frame subset named ‘dfo’. Next, we concatenate the original data frame  <code>df</code>  while dropping those columns selected in the dfo,  <code>df.drop(dfo,axis=1)</code>, with the  <code>pandas.get_dummies(dfo)</code>  command, creating only indicator columns for the selected object data type columns and collating it with other numeric data frame columns.</p>
<p><img src="https://miro.medium.com/max/60/1*BNIG77Kc6sg0y6OcSmI4Fw.png?q=20" alt=""></p>
<p><img src="https://miro.medium.com/max/543/1*BNIG77Kc6sg0y6OcSmI4Fw.png" alt=""></p>
<p>Dummies now added to the data frame with column name such as ‘Manufacturer_’</p>
<p>We perform this conversion regardless of the type of machine learning model we plan on developing because it allows a standardized data set for model development and further data manipulation should our planned approach not provide excellent results in the first pass. Pre-processing is the concept of standardizing your model development dataset.</p>
<hr>
<p><strong>2. Standardize the magnitude of numeric features</strong></p>
<p>This is applied in situations where you have differences in the magnitude of numeric features. This would also be the juncture where other numeric translation would be applied to meet some scientific assumptions about the feature, such as accounting for atmospheric attenuation in satellite imagery data. However, you do not pass your dummy aka indicator features to the scaler; they do not need to be scaled as they as are boolean representations of categorical features.</p>
<blockquote>
<p>Many machine learning algorithms objective functions are based on the assumption that the variables have mean of zero and have variance in the same order of magnitude of one, think L1 and L2 regularization. If the development features are not standardized then the larger magnitude features may dominate the objective function and further may spuriously reduce the impact of other features in the model.</p>
</blockquote>
<p>Here is an example, the below data is also from the automobile sales dataset. You can see from the distribution plots for each feature that they vary in magnitude.</p>
<p><img src="https://miro.medium.com/max/60/1*viPzBC23avFeO3KVzjceBw.png?q=20" alt=""></p>
<p><img src="https://miro.medium.com/max/1043/1*viPzBC23avFeO3KVzjceBw.png" alt=""></p>
<p>Numeric Features of differing magnitudes</p>
<p>When applying a scaler transformation we must save the scaler and apply the same transformation to the testing data subset. Therefore we apply it in two steps, first defining the scaler based on the mean and standard deviation of the training data and then applying that scaler to each the training and testing sets.</p>
<hr>
<p><strong>3. Split the data into training and testing subsets</strong></p>
<p>Implementing a data subset approach with a train and test split with a 70/30 or 80/20 split is the general rule for an effective holdout test data for model validation. Review the code snippets and the other considerations below on splitting the model development data set into training and testing subsets.</p>
<hr>
<p><strong>Other Considerations in training data development</strong></p>
<p>If you have time series data be sure to consider how to appropriately split your data into training and testing data for your optimal model outcome. Most likely you’re looking to forecast, and your testing subset should probably consist of the time most recent to the expected forecast or for the same period in a different year, or something logical for your particular data. An example of date specified splitting function is provided here:</p>
<p>Additionally, if your data needs to be stratified during the testing and training data split, such as in our example if we considered European carmakers to be different strata then American carmakers we would have included the argument  <code>stratify=country</code>  in the train_test_split command.</p>
<p>In upcoming articles, I’ll cover machine learning model fitting and model review. Stay in touch by following me on Medium or Twitter. To receive updates about the Data Science Method Sign up  <a href="https://www.breakthroughdatascience.com/pl/102737">here</a>.</p>
<h4 id="keywords">Keywords</h4>
<ul>
<li><a href="https://medium.com/tag/machine-learning">Machine Learning</a></li>
<li><a href="https://medium.com/tag/data-preprocessing">Data Preprocessing</a></li>
<li><a href="https://medium.com/tag/data-science">Data Science</a></li>
<li><a href="https://medium.com/tag/professional-development">Professional Development</a></li>
<li><a href="https://medium.com/tag/training-data">Training Data</a></li>
</ul>
</div>
</body>

</html>
