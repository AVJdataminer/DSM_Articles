
# The Data Science Method (DSM)-Data Wrangling

[](https://medium.com/@aiden.dataminer?source=post_page-----d19b6ff141c4----------------------)

![Aiden V Johnson](https://miro.medium.com/fit/c/48/48/1*WDVszdcJs5A1fXhnjHZPig.jpeg)

[Aiden V Johnson](https://medium.com/@aiden.dataminer?source=post_page-----d19b6ff141c4----------------------)

[Dec 10, 2018](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-data-collection-organization-and-definitions-d19b6ff141c4?source=post_page-----d19b6ff141c4----------------------)  ·  6  min read

This is the second article in a series about how to take your data science projects to the next level by using a methodological approach similar to the scientific method coined the Data Science Method. This article is focused on number two; Data Wrangling. If you missed the previous article in this series on Problem Identification, go back and read it  [here](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-a-framework-on-how-to-take-your-data-science-projects-to-the-next-91f9fd81e5d1).

[**The Data Science Method**](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-a-framework-on-how-to-take-your-data-science-projects-to-the-next-91f9fd81e5d1?source=friends_link&sk=88ba3e688531910bb00142b0b68df62f)

1.  [Problem Identification](https://medium.com/@aiden.dataminer/the-data-science-method-problem-identification-6ffcda1e5152?source=friends_link&sk=a56dc93c270bfdd21a585b0618984be0)
2.  Data Wrangling
3.  [Exploratory Data Analysis](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-exploratory-data-analysis-bc84d4d8d3f9)
4.  [Pre-processing and Training Data Development](https://link.medium.com/i5yDUwZi9W)
5.  [Modeling](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-modeling-56b4233cad1b)
6.  [Documentation](https://medium.com/@aiden.dataminer/the-data-science-method-dsm-documentation-c92c28bd45e6)

![](https://miro.medium.com/max/30/1*fmbeSY_DBQloeanWDN_MYA.png?q=20)

![](https://miro.medium.com/max/1702/1*fmbeSY_DBQloeanWDN_MYA.png)

----------

**Data Wrangling Steps:**

Data wrangling consists of four high-level steps that should be applied in any data science project.

1.  **Data Collection**
2.  **Data Organization**
3.  **Data Definition**
4.  **Data Cleaning**

**DATA COLLECTION**

Data Collection can vary depending on the scope of the project and the data available. Assuming we have completed Problem Identification, Step 1 in the DSM, then ask yourself if the data you have can answer the question of interest. In some cases, writing website scrapers, scouring for census data and search through other web sites for available data can be a time-consuming but necessary task. Perhaps you have data provided by your client, in that case, you may receive access to an S3 bucket or a SQL database each with many tables or files of data or you could be given a single CSV file. Regardless of the situation, you will likely need to spend some time locating your data.

Once you have procured the datasets you need you’ll want to load them into a friendly format such as a data frame. Additionally, collating all your data sources into a single data frame for analysis will make your project much easier going forward. Depending on the relationship of your data pieces and how they tie together you may need to apply some methods from step 4. Data Cleaning, before you’re ready to concatenate the pieces together into a single data frame.

Collating your data sources at this early point in the workflow allows for clean data processing and easy adjustments in methods at later stages in the work.

----------

**DATA ORGANIZATION - DIRECTORY STRUCTURE**

Directory organization is one of those things that you may not think of much when you’re just starting out in data science but as you get into multiple iterations of the same model having a well-structured environment for your model outputs and potentially intermediate steps and data visualizations is paramount. Containerized approaches may reduce the need for this and highly structured work environments also don’t require an additional organizational directory. The key is to keep things organized, clean, and dated and/or versioned.

![](https://miro.medium.com/max/30/1*h-meTEPnSsoGwc-Hhz6a7A.png?q=20)

![](https://miro.medium.com/max/317/1*h-meTEPnSsoGwc-Hhz6a7A.png)

Here is an example of a simple modeling directory.

It’s clearly lacking creativity with the higher folder names, but what it lacks in creativity it gains in simplicity. The date and time stamps help to provide a simple way to identify previous model iterations. Adding a more descriptive name to each modeling folder is good to such as; RandomForest_500, or RCN_3layers.

As you start to develop more advanced methods you may need a more complex structure. Another common method for project organization in Python is by using the  [cookiecutter python package](https://drivendata.github.io/cookiecutter-data-science/)  to generate the file structure for a new project. Below is what the structure looks like.

![](https://miro.medium.com/max/27/1*MpwB4_5jl9lsrYhgBjMF2g.png?q=20)

![](https://miro.medium.com/max/883/1*MpwB4_5jl9lsrYhgBjMF2g.png)

[https://drivendata.github.io/cookiecutter-data-science/](https://drivendata.github.io/cookiecutter-data-science/)

If you come from a computer science background you may already be familiar with the concept of version control, Git, and Github. Learning how to effectively use Git and repositories is a core skill for all data scientists. More junior data scientists should spend some time reviewing the concepts of version control and expect any established data science team they join to be using it daily.

----------

**DATA DEFINITION**

Data definitions are often a  neglected  piece of a data science project. Sometimes this is considered a model documentation component, however, it is both a documentation as well as a development piece. The process of developing data definitions prior to model development informs the data science practitioner at a glance about their development dataset. The data scientist can quickly review the data dictionary as necessary during the modeling process to refresh their understanding of specific components. The other benefit is in communication with the client during the review of intermediate steps or model reviews. When the data definitions are clear and in writing everyone in the exchange is on the same page about what the data features represent.

The data definition should contain the following items by column:

1.  Column Name
2.  Data Type (numeric, categorical, timestamp, etc)
3.  Description of Column
4.  Count or percent per unique values or codes (including NA)
5.  The range of values or codes

One example of data definitions is what  [kaggle](https://www.kaggle.com/spscientist/students-performance-in-exams)  has on their datasets pages about each downloadable dataset. Below is the header for the table on student’s exam performance with some important data descriptors identified.

![](https://miro.medium.com/max/30/1*j1fHigZCVVQqihY66uU1Qw.png?q=20)

![](https://miro.medium.com/max/761/1*j1fHigZCVVQqihY66uU1Qw.png)

Example of data definition — from kaggle.com

Here is another example where the variable called ‘NETW30’ is described in detail in the sentence above the table. The table defines each column by the unique codes found in that column, the particular description each code represents as well as the count and percent of each unique code in the column. Based on the description we can infer this is a categorical data column that represents the results of a prior statistical model which may have multiplicative error considerations which are good to be aware of.

![](https://miro.medium.com/max/30/1*CGbxsV-J7pmOGOv5BMACmQ.png?q=20)

![](https://miro.medium.com/max/959/1*CGbxsV-J7pmOGOv5BMACmQ.png)

An example data definition table for one column called NETW30.

----------

**DATA CLEANING**

Data Cleaning can be a time-consuming process and can have serious implications for spurious results if not done properly before beginning the modeling project.

The most common types of data cleaning steps are:

1.  Handling missing and NA data
2.  Removing duplicates

As we mentioned in step 1 perhaps you need to combine two data frames from different sources into a single data frame. This requires unifying the formatting and filling in gaps of overlap with NA or other thoughtful fillers such as 0 or 1 when applicable.

The first step in this process is to identify how many NA values are in your data set. This can be done by printing out the ``df.info()`  to get the data type for each column. One may also want to run additional analyses to ensure there are no mask values such as '-9999' or values such as 'none', these missing values will be missed by ``is.null()`, this scenario can be identified by running ``value_counts()`  for each column in your data frame. Once we determine the different missing or NA values in the data we need to handle them appropriately. Reviewing the percentage of missing observations aids in determining the best step forward.

Follow these steps:

1.  Review the percentage of observations missing per column
2.  Drop, Impute, or Replace missing values

If less than one percent of the data column is missing then it can be dropped. For percentages larger than one percent each column would need to be reviewed in detail to determine the appropriate handling method. The main consideration is how impactful will the NA approach be on the overall distribution of the data. Dropping the entire row with a missing value rather than simply a missing observation is the best way to fairly drop NA values from a data frame.

_Drop duplicate rows and columns_

Next, we need to check for duplicate rows and columns. Duplicate rows could be legitimate values depending on your data and how it was collected or the magnitude of variation that is expected in your data. The only time you should delete duplicate rows is if you are confident they are repeated measures of the same observation and that it has negative consequences for your expected statistical modeling method. Your prior work on data definitions will inform you of any duplicate columns. Duplicate columns are common when multiple data sources are combined to create the model development dataset. These may not have the same column name, but if the columns’ rows are identical to another column, one of them should be removed.

----------

As you go through these steps with your work your data science modeling project will transition to a professional standardized approach to completing a data science project. To receive updates about the Data Science Method Sign up  [here](https://www.breakthroughdatascience.com/pl/102737).

#### Keywords

-   [Data Science](https://medium.com/tag/data-science)
-   [Machine Learning](https://medium.com/tag/machine-learning)
-   [Professional Development](https://medium.com/tag/professional-development)
-   [Data](https://medium.com/tag/data)
-   [R](https://medium.com/tag/r)
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTM5OTY0MTIxXX0=
-->